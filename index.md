---
layout: default
title: "Mitigating Director Gender Bias in Movie Recommender Systems"
---

## Introduction

Studies in sociology and media studies have revealed a gender gap in the film industry, namely the underrepresentation of female directors in film production.
  - In over 2000 films released between 1994 and 2016, only 5% of the directors were female (Karniouchina et al., 2023).
  - In top-grossing films released between 2007 and 2021, the ratio of male to female directors was 11:1 (Smith et al., 2017).

However, the implications of this disparity on recommendation systems is not widely researched.
  - Most studies look at bias from a statistical perspective (e.g., popularity bias).

Many content distribution platforms (such as Netflix) utilize recommendation models for personalized user content.
  - A recommender system filters information (e.g., user and item data) to provide personalized suggestions to users.

As such, we investigate whether this gender bias is embedded into the outputs and suggestions of various recommendation models developed using different similarity metrics and algorithms.

Widely adopted bias mitigation tools (such as IBM's AI Fairness 360 open source toolkit) are also optimized for regression and classification tasks, not recommendation tasks.

Our project fills this gap by investigating whether bias mitigation techniques developed for regression and classification tasks can be extended to recommendation systems.

Our aim is to develop a fair movie recommender system that minimizes biases associated with the gender of the director.

## Methods

Your methods content goes here.

## Model Development & Evaluation

Your model development content goes here.

## Bias Mitigation

Your content goes here.

## Conclusion & Results

Your conclusion and discussion content goes here.
